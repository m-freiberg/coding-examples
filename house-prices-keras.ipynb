{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.csv', 'train.csv', 'sample_submission.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense, BatchNormalization, Flatten, LSTM, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(128, input_dim=40, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(layers.Dense(256, kernel_initializer='normal', activation='relu'))\n",
    "#     model.add(layers.Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "#     model.add(Dense(1, kernel_initializer='normal'))\n",
    "#     # Compile model\n",
    "#     model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "    \n",
    "    # define model\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(128, input_shape=(None, 40)))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     # compile model\n",
    "#     model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(4, input_shape=(1, 1)))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM(128, input_shape=((1, 40)), activation='linear', return_sequences=True))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(LSTM(256, activation='linear'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Dense(256, activation='linear'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(128, activation='linear'))\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(64, activation='linear'))\n",
    "#     model.add(Dropout(0.2))\n",
    " \n",
    "    model.add(Dense(32, activation='linear'))\n",
    "#     model.add(Dropout(0.2))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 1)\n",
      "(1460, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path of the file to read. We changed the directory structure to simplify submitting to a competition\n",
    "iowa_file_path = '../input/train.csv'\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "\n",
    "# Create X\n",
    "X = home_data.drop(['SalePrice'], axis=1)\n",
    "X = X.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Split into validation and training data\n",
    "# train_X, test_X, train_y, test_y= train_test_split(X,y,\n",
    "#                                                     train_size=0.7, \n",
    "#                                                     test_size=0.3, \n",
    "#                                                     random_state=0)\n",
    "\n",
    "cols_with_missing = (col for col in X.columns if X[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    X[col + '_was_missing'] = X[col].isnull()\n",
    "#     test_X[col + '_was_missing'] = test_X[col].isnull()\n",
    "\n",
    "# X = X.drop(columns=cols_with_missing)\n",
    "# Imputation\n",
    "features = X.columns\n",
    "my_imputer = SimpleImputer()\n",
    "X = my_imputer.fit_transform(X)\n",
    "# train_X = my_imputer.fit_transform(train_X)\n",
    "# test_X = my_imputer.transform(test_X)\n",
    "# X = np.concatenate((train_X,test_X))\n",
    "# y = np.concatenate((train_y, test_y))\n",
    "min_max_scaler = MinMaxScaler()\n",
    "standard = StandardScaler()\n",
    "# min_max_scaler = standard\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\n",
    "\n",
    "print((X.shape[0], X.shape[1]))\n",
    "print(X.shape)\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "# estimator = KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=-2)\n",
    "\n",
    "# kfold = KFold(n_splits=10, random_state=seed)\n",
    "# results = cross_val_score(estimator, X, y, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "# Key_Features = pd.DataFrame({'feature': list(features),\n",
    "#                    'importance': estimator.feature_importances_}).\\\n",
    "#                     sort_values('importance', ascending = False)\n",
    "\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "\n",
    "\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=80, batch_size=32, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "# kfold = KFold(n_splits=4, random_state=seed)\n",
    "# results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "# print(\"Standardized: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 128 batch sizes\n",
    "# lstm 256 (0.2) --> lstm 128 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [80, 128]:                    -20103.75 (1845.26) MAE\n",
    "# lstm 256 (0.2) --> lstm 128 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [70, 128]:                    -20318.08 (1596.56) MAE\n",
    "# lstm 256 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [70, 128]:                    -20275.79 (1662.84) MAE\n",
    "# lstm 256 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [80, 128]:                    -19831.02 (1896.39) MAE\n",
    "# lstm 512 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [80, 128]:                    -19844.62 (1773.10) MAE\n",
    "# lstm 128 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [80, 128]:                    -20227.35 (1658.87) MAE\n",
    "# lstm 128 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) [100, 128]:                   -19564.36 (1721.07) MAE\n",
    "# lstm 128 (0.2) --> lstm 64 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]:  -20192.38 (2503.99) MAE\n",
    "# lstm 128 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19448.54 (1978.56) MAE\n",
    "# lstm 256 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19745.67 (2118.28) MAE\n",
    "# lstm 256 (0.2) --> lstm 128 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19728.36 (1460.42) MAE\n",
    "# lstm 128 (0.2) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [80, 128]:  -20270.39 (1886.73) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 100 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19352.87 (1786.66) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 128 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19368.60 (1775.76) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19318.86 (2048.19) MAE\n",
    "# softmax activation\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -180918.88 (2514.87) MAE\n",
    "# all relu\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19214.07 (1918.02) MAE\n",
    "# all linear\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19615.55 (1522.68) MAE\n",
    "# standard scaler (all linear)\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]:  -18246.12 (1304.44) MAE\n",
    "# standard scaler (all relu)\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -20008.41 (1320.68) MAE\n",
    "# standard scaler (normal activations)\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 (0.2) -> dense 128 (0.2) -> dense 64 (0.2) -> dense 32 (0.2) [100, 128]: -19576.05 (1553.72) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [100, 128]:                         -18590.97 (1596.56) MAE                     \n",
    "# standard scaler (all linear)\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [100, 128]:                         -17980.04 (1265.17) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [80, 128]:                          -17597.71 (998.45) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [70, 128]:                          -18737.23 (1205.90) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [75, 128]:                          -17778.31 (1084.07) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [85, 128]:                          -17849.28 (1375.23) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [150, 128]:                         -17820.91 (1705.90) MAE\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [80, 64]:                           -17172.10 (1481.64) MAE\n",
    "# not imputing, dropping columns with nulls\n",
    "# lstm 128 (0.1) --> lstm 256 (0.2) -> dense 256 -> dense 128 -> dense 64 -> dense 32 [80, 128]:                          -17016.59 (1400.03) MAE\n",
    "                     \n",
    "\n",
    "# pca = PCA(n_components=40)\n",
    "# pca.fit(X)\n",
    "\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(1)\n",
    "rf_model_on_full_data = pipeline\n",
    "print(2)\n",
    "\n",
    "# cols_with_missing = [col for col in X.columns \n",
    "#                                  if X[col].isnull().any()]\n",
    "# X = X.drop(cols_with_missing, axis=1)\n",
    "\n",
    "# fit rf_model_on_full_data on all data from the \n",
    "rf_model_on_full_data.fit(X, y)\n",
    "print(3)\n",
    "\n",
    "# path to file you will use for predictions\n",
    "test_data_path = '../input/test.csv'\n",
    "\n",
    "# read test data file using pandas\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "cols_with_missing = (col for col in test_data.columns if test_data[col].isnull().any())\n",
    "\n",
    "for col in cols_with_missing:\n",
    "    test_data[col + '_was_missing'] = test_data[col].isnull()\n",
    "\n",
    "# create test_X which comes from test_data but includes only the columns you used for prediction.\n",
    "# The list of columns is stored in a variable called features\n",
    "test_X = test_data[features]\n",
    "\n",
    "\n",
    "# my_imputer = SimpleImputer()\n",
    "test_X = my_imputer.transform(test_X)\n",
    "\n",
    "print(4)\n",
    "test_X = min_max_scaler.transform(test_X)\n",
    "\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "\n",
    "\n",
    "# make predictions which we will submit. \n",
    "preds = rf_model_on_full_data.predict(test_X)\n",
    "print(5)\n",
    "\n",
    "# The lines below shows you how to save your data in the format needed to score it in the competition\n",
    "output = pd.DataFrame({'Id': test_data.Id,\n",
    "                       'SalePrice': preds})\n",
    "\n",
    "output.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_mat = home_data.drop(columns=['Id']).corr()\n",
    "# fig = plt.figure(figsize = (20,1))\n",
    "# sales_corr = pd.DataFrame()\n",
    "# for name in C_mat.columns:\n",
    "#     to_add = np.absolute([C_mat[name][36]])\n",
    "#     if to_add[0] > 0.3 and name != 'SalePrice':\n",
    "#         sales_corr[name] = [C_mat[name][36]]\n",
    "# print(sales_corr.columns)\n",
    "\n",
    "# sb.heatmap(sales_corr, square = True, cmap=\"YlGnBu\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
