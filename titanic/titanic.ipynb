{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport pandas as pd\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as lgbm\n\n\nimport warnings\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv')\n\nlen_train = len(train)\n\n# record survived and drop it to make train/test headers match\nsurvived = train['Survived']\ntrain = train.drop(columns=['Survived'])\n\n# append testing onto training for ease in preprocessing\ntrain = train.append(test, ignore_index = True, sort=False)\n\n# extract titles from names\ntrain['titles'] = train['Name'].str.extract('([a-zA-Z]+)\\.', expand=True)\n# this mapping from https://www.kaggle.com/vincentlugat/titanic-data-analysis-lgbm-0-82296\nmisc_titles = {'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs',\n              'Major':'Misc', 'Col':'Misc', 'Dr':'Misc', 'Rev':'Misc', 'Capt':'Misc',\n              'Jonkheer':'Royal', 'Sir':'Royal', 'Lady':'Royal', 'Don':'Royal', 'Countess':'Royal', 'Dona':'Royal'}\ntrain['titles'] = train['titles'].replace(to_replace=misc_titles)\ntrain = train.join(pd.get_dummies(train['titles']))\n\n# make sex numerical\ntrain['Sex'] = train['Sex'].replace(to_replace = {'male':1, 'female':0})\n\n# look at age distributions in each social class and gender and assign any null ages\n# print(train['Age'].hist(by=[train['Sex'], train['Pclass']], xlabelsize = 10, figsize=(8,10)))\ntrain['Age'] = train.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n\n# replace any nan fares with median of fare grouped by sex and pclass\ntrain['Fare'] = train.groupby(['Sex', 'Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\n\n# extract letter before cabin (associated with placement on ship), put U (unassigned) for NaN's\ncabins = []\nfor cabin in train['Cabin']:\n    if str(cabin) == 'nan': cabins.append('U')\n    else: cabins.append(str(cabin).strip()[0])\ntrain['Cabin'] = cabins\ntrain['Cabin'] = train['Cabin'].replace(to_replace = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9})\n\n# 2 values in embarked that are nan, replace them with 'S' which is the large majority of embarked\n# print(train['Embarked'].value_counts())\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntrain['Embarked'] = train['Embarked'].replace(to_replace = {'S':1, 'C':2, 'Q':3})\n\n# get some new features\ntrain['is_child'] = (train['Age'] < 18).astype(int)\ntrain['is_alone'] = (train['SibSp'] + train['Parch'] == 0).astype(int)\n\n# drop unnecessary columns\ntrain = train.drop(columns=['Name', 'Ticket', 'titles'])\n\n# scale number data\nmin_max_scaler = MinMaxScaler()\ntrain[['Fare', 'Age']] = min_max_scaler.fit_transform(train[['Fare', 'Age']])\n\n# nothing is null anymore\nprint(train.isnull().values.any())\n\n# resplit train and test\ntest = train.iloc[len_train:, :]\nids = test['PassengerId']\ntest = test.drop(columns=['PassengerId'])\n\ntrain = train.iloc[:len_train,:]\ntrain = train.drop(columns=['PassengerId'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# review general feature importance\nmodel = RandomForestClassifier(random_state = 42)\nx = train\ny = survived\nmodel.fit(x, y)\n\nsort = [x for _,x in sorted(zip(list(model.feature_importances_),x.columns), reverse=True)]\ny_vis = sorted(list(model.feature_importances_), reverse=True)\nplt.bar(sort[:32], y_vis[:32], align='center', alpha=0.5)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize random forest\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 80, 100],\n    'max_features': [5, 10, 16],\n    'min_samples_leaf': [1, 4, 7],\n    'n_estimators': [10, 100, 200],\n    'random_state': [16, 42, 73]\n}\n\nrf = RandomForestClassifier()\n\n# complete grid search\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 10, n_jobs = -1, verbose = 2)\n# fit and get best\ngrid_search.fit(x, y)\nmodel_rf = grid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize logistic regression\nparam_grid = {\n    'penalty': ['l2', 'l1'],\n    'C': [0.5, 1.0, 1.5],\n    'tol': [0.000001, 0.00001, 0.0001, 0.001, 0.01],\n    'random_state' : [15, 33, 42, 72],\n    'solver' : ['saga', 'warn'],\n    'max_iter' : [50, 100, 150, 200]\n}\n\nlr = LogisticRegression()\n\n# compelete grid search\ngrid_search = GridSearchCV(estimator = lr, param_grid = param_grid, \n                          cv = 10, n_jobs = -1, verbose = 2)\n# fit and get best\ngrid_search.fit(x, y)\nmodel_lr = grid_search.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize knn classifier\nparam_grid = {\n    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'n_neighbors': [1, 3, 5, 7],\n    'leaf_size': [5, 15, 30, 45],\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski' ]\n}\n\nknn = KNeighborsClassifier()\n\n# complete grid search\ngrid_search = GridSearchCV(estimator = knn, param_grid = param_grid, \n                          cv = 10, n_jobs = -1, verbose = 2)\n# fit and get best\ngrid_search.fit(x, y)\nmodel_knn = grid_search.best_estimator_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stack and find accuracies of individual and overall\nwarnings.simplefilter('ignore')\n\nRANDOM_SEED = 42\n\nlgbm_clf = lgbm.LGBMClassifier(random_state=RANDOM_SEED)\n\nsclf = StackingCVClassifier(classifiers=[model_knn, model_rf, model_lr, lgbm_clf],\n                            meta_classifier=LogisticRegression(random_state=RANDOM_SEED),\n                            random_state=RANDOM_SEED)\n\nfor clf, label in zip([model_knn, model_rf, model_lr, lgbm_clf, sclf], \n                      [ \n                        'KNN',\n                        'Random Forest', \n                        'Logistic Regression',\n                        'LGBM',\n                        'StackingClassifier' ]):\n\n    scores = model_selection.cross_val_score(clf, x, y, \n                                              cv=10, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n          % (scores.mean(), scores.std(), label))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit stacking classifier on whole data set and predict test values\n\nsclf.fit(x, y)\n\ndf_results = pd.DataFrame()\npred = sclf.predict(test)\n\ndf_results['PassengerId'] = ids\ndf_results['Survived'] = pred\n\ndf_results.to_csv('titanic_submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
